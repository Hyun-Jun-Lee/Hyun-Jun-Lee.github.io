---
title:  "[Section1] 선형대수 활용 및 PCA"
excerpt: "AIB 컨텐츠 복습 챌린지"

categories:
- AIB2 Study

toc: True
toc_sticky: True

date: 2021-10-14
last_modified_at: 2021-10-14
---

# [Section1] 선형대수 활용 및 PCA

<br>

## Linear Projection

- 도형이나 입체를 다른 평면에 옮기는 일
- 고유벡터(eigen vector) 직선위에 모든 데이터를 올려두는 것

## 고유값(eigen value)와 고유벡터(eigen vector)

```
벡터 v에 대해 선형 변환 A를 해주었을 때, 벡터 v의 방향은 변하지 않고 크기만 변하고 변한 크기가 원래 벡터 크기의 x배라면, 그 `x를 고유값` 이라고하고 `v를 고유벡터`라고합니다.
```

<br>

보통 어떤 벡터에 선형 변환을 하면 방향이 바뀝니다. 하지만 선형 변환을 했음에도 어떤 `벡터 v의 방향이 바뀌지 않고 크기만 변했다면`, 그리고 변한 크기가 원래 벡터 크기의 λ배라면 그 `λ를 고유값`이라고 하고 `v는 고유 벡터`라고 합니다.

<br>

- SVD(특이값 분해), PCA(주성분 분석) 등의 응용이 고유값, 고유벡터를 바탕으로 이루어지기 때문에 중요

<br>

### 선형 변환(Linear transformation)

- Vector에 사칙연산을 해주는 개념
  - ex)  벡터 v에 행렬 A를 곱하는 것을 '열 벡터 v에 선형 변환 A를 해주었다.'라고 말할 수 있음
  
<br>

- 선형(Linear) : 어떤 시스템의 입력 값에 영향을 준 만큼 출력 값도 그만 큼만 영향을 받는 시스템
  - ex) y=3x -> y는 3을 곱한 만큼만 변함 / y=3x^2 -> 2를 곱햇는데 겨로가 값은 다르게 변함
    
<br>

### 행렬의 대각화(Diagonalization)와 고유값 분해(eigenvalue decomposition)

- 행렬의 대각화(Diagonalization)
  - 고유값과 고유벡터를 구하기 위해 행렬을 대각행렬(대각성분 제외 모두0인 행렬)로 만드는 방법

- 고유값 분해(eigenvalue decomposition)
  - 행렬을 분해해 고유값과 고유벡터로 표현하는것
  - 대각화가 가능한 n x n 정방행렬에만 사용 가능

- 특이값 분해(Singular Value Decomposition, SVD)
  - 고유값 분해의 일반화 버전으로 모든 행렬에 사용 가능
  - 특이값 : 분산과 표준편차 처럼, 고유 값에 루트를 씌운 것

<br>

```
고유값 분해는 대칭행렬 A에 대해 A를 고유값, 고유벡터를 이용해 분해하는 것
특이값 분해는 임의의 행렬 A에 대해 ATA,AAT A T A , A A T 의 고유값, 고유벡터를 이용해 분해하는 것
```

<br>

## 차원의 저주(Curse of Dimensionality)

- 차원
  - 독립 변수의 개수
  - ex) nana = [52,165,2,33,98] -> 5차원

- 차원의 저주
  - 고차원에서 균일하게 데이터가 분포해 있으려면 늘어난 차원 수 보다 더 많은 데이터 필요
  - 데이터가 고차원(=feature가 많아)이라 훈련 데이터가 충분히 전체 공간에 나타나지 못하면 과적합(overfitting) 위험이 있고 개별 feature간의 상관관계가 높을 가능성이 있음
  - 해결책
    1. 고차원 공간에 균일하게 분포할 수 있을 만큼 데이터 늘리기
    2. 차원 축소(Dimensionality Reduction)

<br>

## 차원축소와 PCA

### 차원 축소

- 많은 feature로 구성된 다차원 데이터 세트의 차원을 축소해 새로운 차원의 데이터 세트를 생성하는 것
- 차원축소를 하는 이유
  1. 시각화(Visualization)
     - 3차원이 넘어가면 눈으로 판단 못하기 때문에, 차원 축소를 통해 데이터 패턴을 쉽게 인지할 수 있게 시각화
  2. 노이즈 제거(Reduce Noise)
     - 쓸모없는 feature 제거해 noise 제거
  3. 메모리 절약 (Preserve useful info in low memory)
  4. 퍼포먼스 향상

<br>

### PCA(주성분분석)

- 고차원의 데이터를 저차원으로 축소시키는 방법 중 하나로, 여러 feature 중 결과에 중요한 영향을 끼치는 feature만 선택하는 것

<br>

- PCA Process
  - 데이터에서 의미있는 축을 찾는 과정, 각각의 축은 하나의 주성분에 해당(pc1,pc2)
  - 데이터에는 차원 수 만큼의 주성분이 존재하는데, 여기서 PCA는 어떤 축이 중요한지 그 우선 순위를 구하는 것
  - <a href="https://bkshin.tistory.com/entry/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-9-PCA-Principal-Components-Analysis">Blog 참고</a>

<br>

- PCA의 단점
  - 주성분은 원본 데이터에 있는 어떤 방향에 대응하는 여러 특성이 같이 조합된 형태라서, 거의 대부분의 특성이 섞여있기 때문에 축이 가지는 의미를 설명하기가 어려움

