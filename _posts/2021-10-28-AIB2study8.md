---
title:  "[Section2] classification"
excerpt: "AIB 컨텐츠 복습 챌린지"

categories:
- AIB2 Study

toc: True
toc_sticky: True

date: 2021-10-28
last_modified_at: 2021-10-28
---
# [Section2] Classification

```
1. 결정트리, 랜덤포레스트, 앙상블 키워드 위주로 개념을 공부하기
2. 결정트리의 장/단점, 사용하는 이유에 대해서 알아보기
3. 결정트리에서 확인 가능한 특성중요도가 무엇인지 알아보기
4. 결정트리모델과 선형모델과 어떤 차이가 있는지 알아보기
5. 결정트리와 과적합에 대해서 공부해보기 (아래에 자료 첨부해뒀습니다~)
```

<br>

## Classification

### Decision Tree

- Classification/Regression 모두 가능한 지도 학습 모델
- 특정 기준에 따라 데이터를 구분

![image](https://user-images.githubusercontent.com/76996686/139271387-b98ddebe-e45c-4b50-846c-dd694ab411bf.png)

<br>

#### Impurity

- 분기 기준을 선택하기 위한 개념으로 복잡성을 의미, 해당 범주안에 서로 다른 데이터가 얼마나 섞여 있는지를 뜻함
- 분기 기준 설정 시, 현재 노드의 impurity에 비해 자식 노드의 impurity가 감소하도록 설정해야함
  - 현재 노드와 자식 노드의 impurity 차이를 Imformation gain
- Gini 지수
  - `1 - 전체 데이터 개수 중 각 레이블이 차지하는 개수의 비율`
  - 값이 작을 수록 분할이 잘 되었다는 것을 뜻함
  - ex) 총 4개의 데이터 세트에서 레이블 A가 3개, B가 1개 일 때<br><br>
  ![image](https://user-images.githubusercontent.com/76996686/139274425-fbc97895-d187-412c-af72-3365c3c1fa36.png)

<br>

#### Pruning

- Decision Tree의 depth가 깊어 과접합이 발생하는 것을 방지하기 위해 불필요한 가지를 제거하는 것

#### Prons and Cons

- 장점
  - 해석이 쉬움
  - 두개 이상의 변수가 결합하여 타겟 변수에 어떻게 영향을 주는지 쉽게 알 수 있음
  - 비모수적 모형으로 선형성, 정규성 등의 가정 필요 X
- 단점
  - 연속형 변수를 비연속적인 값으로 취급을 해서 예측 오류 클 가능성
  - 각 변수의 영향력을 알 수 없음
  - 예측에서는 불안정

<br>

#### 하이퍼파라미터

- `criterion` : 노드 분류 기준, 'gini' or 'entropy'
- `splitter` : 노드 분리 방법, 'random' or 'best'
- `max_depth` : 트리의 최대 깊이 지정하여 과적합 방지
- `min_samples_split` : 중간 노드에서 분리가 일어나기 위한 샘플 수
- `min_samples_leaf` : leaf노드에서 필요한 최소 샘플 수, 너무 적으면 과적합 발생 가능
- `max_feature` : 노드 분리할 때 고려하는 속성 수

<br>

### Random Forest

- 훈련을 통해 구성해놓은 Decision Tree로부터 분류 결과를 취합해서 결론을 얻는 방식(Ensemble)
- 각 Decision Tree 훈련 과정에서 Bagging 적용

<br>

#### Bagging & Boosting

- `Bagging` : 원 자료로부터 여러 번의 복원 샘플링을 통해 예측 모형의 분산을 줄여줌으로써 예측력을 향상시키는 방법(중복허용)
- `Boosting` : 약한 예측 모형들의 학습 에러에 가중치를 두고, 순차적으로 다음 학습 모델에 반영하여 강한 예측 모형을 만드는 방법

<br>

#### Prons and Cons

- 장점
  - Decision Tree를 기반으로 각 트리를 만들어 결과를 종합하기만 되서 매우 간단한 알고리즘
  - Overgitting 방지 효과 Good
- 단점
  - 여러 Decision Tree를 만들어야 해서 메모리 사용량 많음
  - train data를 늘려도 성능 변화 적음

<br>

- `n_estimators` : 결정트리 갯수 지정(Default=10)
- `min_samples_split` : 중간 노드에서 분리가 일어나기 위한 샘플 수(작게 설정할 수록 분할 노드가 많아져 과적합 가능성 증가)
- `min_samples_leaf` : leaf가 되기 위해 필요한 최소한의 샘플 데이터수, 너무 적으면 과적합 발생 가능
- `max_features` : 최적의 분할을 위해 고려할 최대 feature 개수
- `max_depth` : 트리의 최대 깊이 지정하여 과적합 방지
- `max_leaf_nodes` : leaf node 최대 개수