---
title:  "[Section2] classification"
excerpt: "AIB 컨텐츠 복습 챌린지"

categories:
- AIB2 Study

toc: True
toc_sticky: True

date: 2021-10-28
last_modified_at: 2021-10-28
---
# [Section2] Classification

<br>

## Classification

### Decision Tree

- Classification/Regression 모두 가능한 지도 학습 모델
- 특정 기준에 따라 데이터를 구분

![image](https://user-images.githubusercontent.com/76996686/139284048-7fc6e1be-224c-44da-9398-7a73a414b570.png)

<br>

#### Impurity

- 분기 기준을 선택하기 위한 개념으로 복잡성을 의미, 해당 범주안에 서로 다른 데이터가 얼마나 섞여 있는지를 뜻함
- 분기 기준 설정 시, 현재 노드의 impurity에 비해 자식 노드의 impurity가 감소하도록 설정해야함
- 현재 노드와 자식 노드의 엔트로피 차이를 Information gain
  - 엔트로피
    - 데이터 집합의 혼잡도, 서로 다른 값이 섞여 있으면 높고 반대는 낮음
    - Information gain = 1 - 엔트로피
- Gini 지수
  - `1 - 전체 데이터 개수 중 각 레이블이 차지하는 개수의 비율`
  - 값이 작을 수록 분할이 잘 되었다는 것을 뜻함
  - ex) 총 4개의 데이터 세트에서 레이블 A가 3개, B가 1개 일 때<br><br>
  ![image](https://user-images.githubusercontent.com/76996686/139274425-fbc97895-d187-412c-af72-3365c3c1fa36.png)

<br>

#### Pruning

- Decision Tree의 depth가 깊어 과접합이 발생하는 것을 방지하기 위해 불필요한 가지를 제거하는 것

#### Prons and Cons

- 장점
  - 해석이 쉬움
  - 두개 이상의 변수가 결합하여 타겟 변수에 어떻게 영향을 주는지 쉽게 알 수 있음
  - 비모수적 모형으로 선형성, 정규성 등의 가정 필요 X
- 단점
  - 연속형 변수를 비연속적인 값으로 취급을 해서 예측 오류 클 가능성
  - 각 변수의 영향력을 알 수 없음
  - 예측에서는 불안정

<br>

#### 하이퍼파라미터

- `criterion` : 노드 분류 기준, 'gini' or 'entropy'
- `splitter` : 노드 분리 방법, 'random' or 'best'
- `max_depth` : 트리의 최대 깊이 지정하여 과적합 방지
- `min_samples_split` : 중간 노드에서 분리가 일어나기 위한 샘플 수
- `min_samples_leaf` : leaf노드에서 필요한 최소 샘플 수, 너무 적으면 과적합 발생 가능
- `max_feature` : 노드 분리할 때 고려하는 속성 수

<br>

### Random Forest

- 훈련을 통해 구성해놓은 Decision Tree로부터 분류 결과를 취합해서 결론을 얻는 방식(Ensemble)
- 각 Decision Tree 훈련 과정에서 Bagging 적용

<br>

#### Prons and Cons

- 장점
  - Decision Tree를 기반으로 각 트리를 만들어 결과를 종합하기만 되서 매우 간단한 알고리즘
  - Overgitting 방지 효과 Good
- 단점
  - 여러 Decision Tree를 만들어야 해서 메모리 사용량 많음
  - train data를 늘려도 성능 변화 적음

<br>

#### 하이퍼파라미터

- `n_estimators` : 결정트리 갯수 지정(Default=10)
- `min_samples_split` : 중간 노드에서 분리가 일어나기 위한 샘플 수(작게 설정할 수록 분할 노드가 많아져 과적합 가능성 증가)
- `min_samples_leaf` : leaf가 되기 위해 필요한 최소한의 샘플 데이터수, 너무 적으면 과적합 발생 가능
- `max_features` : 최적의 분할을 위해 고려할 최대 feature 개수
- `max_depth` : 트리의 최대 깊이 지정하여 과적합 방지
- `max_leaf_nodes` : leaf node 최대 개수

<br>

### Ensemble 학습

- 여러 개의 알고리즘을 사용하여, 그 예측을 결함함으로써 보다 정확한 예측을 도출하는 기법

#### Ensemble 학습 유형

- `Voting` : 여러 종류의 알고리즘을 사용한 각각의 결과에 대해 투표를 통해 최종 결과를 예측하는 방식
  - Hard Voting : 다수결의 원칙
  - Soft Voting : 각 알고리즘이 레이블 값 결정 확률을 예측해서, 이것을 평균하여 이들 중 확률이 가장 높은 레이블 값을 최종 값으로 예측
- `Bagging` : 원 자료로부터 여러 번의 복원 샘플링을 통해 예측 모형의 분산을 줄여줌으로써 예측력을 향상시키는 방법(중복허용)
- `Boosting` : 약한 예측 모형들의 학습 에러에 가중치를 두고, 순차적으로 다음 학습 모델에 반영하여 강한 예측 모형을 만드는 방법
- `Stacking` : 여러 가지 다른 모델의 예측 결과값을 다시 학습 데이터로 만들어 다른 모델(메타 모델)로
재학습시켜 결과를 예측하는 방법