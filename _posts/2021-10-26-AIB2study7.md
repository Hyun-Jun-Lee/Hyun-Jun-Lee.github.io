---
title:  "[Section2] 선형/로지스틱 회귀"
excerpt: "AIB 컨텐츠 복습 챌린지"

categories:
- AIB2 Study

toc: True
toc_sticky: True

date: 2021-10-24
last_modified_at: 2021-10-24
---
# [Section2] 선형/로지스틱 회귀

<br>

## 선형/ 로지스틱 회귀

### 회귀 vs 분류

![image](https://user-images.githubusercontent.com/76996686/138236555-42f79457-8466-4445-bc61-79d27652378f.png)

<br>

### Dataset 분리

- Overfitting 방지를 위해 Dataset을 train/test로 분리
- 모델링의 목표는 Test 데이터를 잘 맞추는 것

1. Dataset -> Train/Test
2. Train -> Train/Validation
3. Train 데이터로 모델 학습 후, Validation으로 검증
4. 검증 후 Train과 Validation을 합쳐서 학습 후 Test 데이터 확인
   - 기존 training set만을 사용하였던 모델의 파라미터와 구조는 그대로 사용하지만, 전체 데이터를 사용하여 다시 학습시킴으로써 모델이 조금 더 튜닝되도록 함

<br>

> Validation 나누는 이유

- Validation set(검정 데이터)은 training set으로 만들어진 모델의 성능을 측정하기 위해 사용
-  일반적으로 어떤 모델이 가장 데이터에 적합한지 찾아내기 위해서 다양한 파라미터와 모델을 사용해보게 되며, 그 중 validation set으로 가장 성능이 좋았던 모델을 선택
-  이후 Test set으로 마지막 확인 필요

<br>

> Validation set vs Test set

- Validation set -> 여러 모델들 각각에 적용되어 성능 측정하여 최종 모델 선정
- Test set -> validation을 통해 선정된 최종 모델에 대해, 앞으로 기대되는 성능 예측위해 사용

<br>

- `train_test_split`

```python
from sklearn.model_selection import train_test_split

# train-test분리
X_train, X_test, y_train, y_test = train_test_split(df['feature'],df['target'])

# train-validation분리
X2_train, X2_val, y2_train, y_val = train_test_split(X_train, y_train)
```

- `test_size` : 테스트 셋 구성의 비율(default값은 0.25)
- `shuffle` : split을 해주기 이전에 섞을것인가 의 여부(default = True)
- `stratify` : classification을 다룰때에는 매우 중요한 옵션값. stratify 값을 target으로 지정해주면 각각의 class비율(ratio)을 train/validation에 유지해 줌 (한쪽에 쏠려 분배되는것을 방지)
- `random_state`: 세트를 섞을때 해당 int값을 보고 섞으며, 하이퍼 파라미터를 튜닝시 이 값을 고정해두고 튜닝해야 매번 데이터셋이 변경되는 것을 방지

<br>

### 과적합/과소적합

![image](https://user-images.githubusercontent.com/76996686/138817265-92613504-591c-46ac-8ff7-158e03273c53.png)

<br>

#### Overfitting

- 너무 복잡한 모델을 생성하는 바람에 학습 데이터에는 굉장히 잘 맞지만 새로운 데이터에는 잘 맞지 않는 현상
- 이런 경우 발생하는 오차를 `분산(Variance)` 이라고 함

<br>

#### Underfitting

- 너무 단순한 모델을 생성하여 학습 데이터와 잘 맞지 않는 현상
- 이런 경우 발생하는 오차를 `편향(Bias)` 이라고 함

<br>

#### Bias-Variance Trade off

![image](https://user-images.githubusercontent.com/76996686/138821439-f9fc8bec-6d3a-43ad-bf5c-03e55786648e.png)

- 일반적으로 Bias와 Variance는 Trade OFF 관계 -> Bias가 높아지면(Underfitting) Variance 낮아짐/Variance가 높아지면(Overfitting) Bias가 낮아짐

<br>

#### Overfitting 방지 

1. 데이터 양 늘리기
   - 모델은 데이터의 양이 적을 경우, 해당 데이터의 특정 패턴이나 노이즈까지 쉽게 암기하기 되므로 과적합 현상이 발생할 확률이 늘어납니다. 그렇기 때문에 데이터의 양을 늘릴 수록 모델은 데이터의 일반적인 패턴을 학습하여 과적합을 방지함.
   - 데이터의 양이 적을 경우에는 의도적으로 기존의 데이터를 조금씩 변형하고 추가하여 데이터의 양을 늘리기도 하는데 이를 데이터 증식 또는 증강(Data Augmentation)이라고 함
2. Model 복잡도 줄이기
3. Regularization 적용하기
   - L1 norm : 가중치 w들의 `절대값 합계`를 비용함수에 추가, 가중치의 절대값을 최소화하는 것을 목표
   - L2 norm : 가중치 w들의 `제곱합`을 비용함수에 추가, 가중치의 제곱을 최소화하는 것을 목표
4. Dropout

<br>

### 원핫인코딩(One-hot encoding)

- 단어 집합의 크기를 벡터의 차원으로 하고, 표현하고 싶은 단어의 인덱스에 1의 값을 부여하고, 다른 인덱스에는 0을 부여하는 단어의 벡터 표현 방식

### Linear Regression

- 종속변수 Y와 한 개 이상의 독립변수 X와의 선형 상관관계를 모델링
  - x의 개수에 따라 단순 선형 회귀, 다중 선형 회귀
  - 독립 변수 : 입력값,원인 / 종속변수 : 출력값, 결과
- ML에서 Regression은 어떻게 분류가 되는지 알아야하고 잘 분류되는지 성능을 파악하여야 하기 때문에 레이블을 이용한 지도학습 필요
- 학습에 이용할 데이터로 모델을 돌린 후 만들어진 회귀식으로 새로운 데이터 값을 예측

#### 회귀식

- `y = βx + ϵ`
  - `β`는 회귀계수, `ϵ`는 종속 변수와 독립 변수 사이에 오차 -> 이 두값이 추정해야할 파라미터
  - x와 y에 해당하는 데이터가 있을 때, 이러한 데이터로부터 β와 ϵ를 추측한 후 추측한 값들을 바탕으로 모델링
  - 그 다음 해당 모델을 기반으로 새로운 데이터의 x값들을 입력으로 넣어주었을 때, 그에 해당하는 y값을 예측

#### 머신러닝 회귀모델 표기법

- `H = Wx + b`
  - H : 가정(Hypothesis)
  - W : 가중치(Weight)
  - b : 편항(Bias)

- 선형 회귀식에서의 회귀계수 β값이 W , 오차 값 ϵ가 b 

<br>

#### 최소제곱법

![image](https://user-images.githubusercontent.com/76996686/138831865-772b2d07-7012-48e6-b44e-0f7b2561927b.png)

- 회귀 분석에서 사용되는 표준 방식, 잔차가 가장 적은 파라미터를 찾는것
- n 개의 점 데이터에 대하여 잔차의 제곱의 합을 최소로 하는 W, b를 구하는 방법

> Regression 평가지표 R-squared(R2) : 0에 가까울수록 설명력이 낮고, 1에 가까울수록 회귀모델의 설명력이 높다

<br>

### Logistic Regression

![image](https://user-images.githubusercontent.com/76996686/138832570-9458315c-7a52-4e43-8bdd-8d2a21e88719.png)

- 종속변수가 binary 일때 사용
- 데이터가 어떤 범주에 속할 확률을 0에서 1 사이의 값으로 예측하고 그 확률에 따라 가능성이 더 높은 범주에 속하는 것으로 분류해주는 지도 학습 알고리즘

<br>

#### 활성화 함수

- Sigmoid 함수 
  - 로지스틱 회귀에 주로 사용, 시그모이드 함수는 결과 값을 0,1로 반환
  - binary classification에 유용

