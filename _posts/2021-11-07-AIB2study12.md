---
title:  "[Section4] 인공신경망 / MLP / 경사하강법 / 오차역전파"
excerpt: "AIB 컨텐츠 복습 챌린지"

categories:
- AIB2 Study

toc: True
toc_sticky: True

date: 2021-11-07
last_modified_at: 2021-11-07
---
# [Section4] 인공신경망 / MLP / 경사하강법 / 오차역전파

<br>

## 퍼셉트론

- 신경망의 기원이 되는 알고리즘
- 다수의 신호를 입력받아 하나의 신호를 출력

<br>

![image](https://user-images.githubusercontent.com/76996686/140649369-a38d1b4a-9b08-464d-a39c-264f306840bd.png)

- x는 입력신호, y는 출력신호, w는 가중치, 원을 뉴런or노드 라고 한다.
- 입력신호가 뉴런에 보내질 때 각각의 고유한 가중치가 곱해짐(x1w1, x2w2)
- 뉴런에서는 전달 받은 신호의 총 합이 특정 임계값을 넘을 때만 1 출력

![image](https://user-images.githubusercontent.com/76996686/140649488-54b4897b-069b-4c4b-8873-7fa518aa92ce.png)

<br>

### 논리 게이트

- AND gate

```python
def AND(x1, x2):
    x = np.array([x1, x2])
    w = np.array([0.5, 0.5])
    b = -0.7
    tmp = np.sum(w*x) + b
    if tmp <= 0:
        return 0
    else:
        return 1
```

- NAND gate

```python
def NAND(x1, x2):
    x = np.array([x1, x2])
    w = np.array([-0.5, -0.5])
    b = 0.7
    tmp = np.sum(w*x) + b
    if tmp <= 0:
        return 0
    else:
        return 1
```

- OR gate

```python
def OR(x1, x2):
    x = np.array([x1, x2])
    w = np.array([0.5, 0.5])
    b = -0.2
    tmp = np.sum(w*x) + b
    if tmp <= 0:
        return 0
    else:
        return 1
```

- XOR gate(multi-layer perceptron)

```python
def XOR(x1, x2):
    s1 = NAND(x1, x2)
    s2 = OR(x1, x2)
    y = AND(s1, s2)
    return y
```

<br>

## 신경망

![image](https://user-images.githubusercontent.com/76996686/140664476-dfdfe674-3ff3-4107-b3ea-0ae02f34dcde.png)

1. 입력신호(X0,X1,X2..)가 입력되면 각각 가중치(W0,W1,W2..)가 곱해짐
2. 다음 노드에서 입력되는 모든 신호를 더해줌(∑)
3. 각 노드에서 활성화 함수에 의한 연산값이 정해진 임계값(TLU)를 넘을 경우에만 다음 layer로 신호 전달

<br>

- Input Layer
  - 데이터를 입력 받는 layer
  - 입력 변수(feature)의 수 = 입력 노드의 수
  - 신경망의 depth를 셀 때 제외되고, 연산없이 그냥 값을 다음 layer에 전달하기만 함

- Hidden Layer
  - Input layer에 입력된 신호가 weigh, bais와 연산되는 층
  - 일반적인 Deep learning은 2개이상의 Hidden layer를 갖춘 신경망을 뜻함


- Output Layer
  - 신경망 외부로 출력 신호 전달하는 역활
  - Output layer의 구조 설계에 따라 신경망의 기능이 결정
    - Binary Classification : Sgmoid 함수를 활성화 함수로 사용하여 Output layer의 노드 수는 1개, 출력 값은 0~1 사이의 확률 값
    - Multi-class Classification : Softmax 함수를 활성화 함수로 사용, Output layer의 노드 수는 레이블의 class 수와 동일
    - Regression : 일반적으로는 활성화 함수를 지정해주지 않으며 출력층의 노드 수는 출력값의 특성(Feature) 수와 동일하게 설정

<br>

```python
model = tf.keras.models.Sequential([
  tf.keras.layers.Flatten(input_shape=(28, 28)), # Flatten : 입력 이미지를 1차원 배열로 변환
  tf.keras.layers.Dense(300, activation='relu'),
  tf.keras.layers.Dense(100, activation='relu'),
  tf.keras.layers.Dropout(0.2), # Overfitting 방지
  tf.keras.layers.Dense(10, activation='softmax')
])


model.compile(optimizer='sgd', # 기본적인 경사하강법 사용하여 모델 훈련한다는 의미
              loss='sparse_categorical_crossentropy', # 이진분류는 'binary_crossentorpy'
              metrics=['accuracy'])
```

<br>

## Backpropagation

- 내가 뽑고자 하는 target값과 실제 모델이 계산한 output이 얼마나 차이가 나는지 구한 후 그 오차값을 다시 뒤로 전파해가면서 각 노드가 가지고 있는 변수들을 갱신하는 알고리즘
- ex) 자유투 연습 때 자유투를 던지는 과정은 Fedd Forward, 던진 공이 어느 지점에 도착하는지 확인 후 자세를 수정하는 과정을 Backpropagation

https://gomguard.tistory.com/182