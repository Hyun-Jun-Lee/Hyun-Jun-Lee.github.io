---
title:  "[Section2] classification"
excerpt: "AIB 컨텐츠 복습 챌린지"

categories:
- AIB2 Study

toc: True
toc_sticky: True

date: 2021-10-31
last_modified_at: 2021-10-31
---
# [Section2] Classification 평가

```
1. 정확도, 오차행렬, 정밀도, 재현율, f1 score의 개념에 대해서 알아보자.
2. ROC, AUC 그래프를 봤을 때 어떤 결과가 더 좋은지판단해보자.
```

<br>

## Classification 평가

### Confusion Matrix

![image](https://user-images.githubusercontent.com/76996686/139582072-ef2b8fca-a046-4467-9303-9ab374c6484c.png)

- True Positive(`TP`) : 실제 데이터가 P(긍정)인데 모델도 P라고 잘 예측했을 때
- True Negative(`TN`) : 실제 데이터가 N(부정)인데 모델도 N이라고 잘 예측했을 때
- False Positive(`FP`) : 실제 데이터가 N인데 모델은 P라고 잘못 예측했을 때
- False Negative(`FN`) : 실제 데이터가 P인데 모델은 N이라고 잘못 예측했을 때

<br>

- Accuracy
  - 얼마나 올바르게 예측하엿나, 
  - `(TP+TN) / (TP+TN+FP+FN)` = 올바르게 예측된 데이터수 % 전체 데이터 수

- Recall(=TPR, 재현율)
  - '실제 P인 데이터 중에서' 모델이 P라고 잘 예측한 데이터의 비율
  - `TP/(TP+FN)`
  - Precision과 Trade-off 관계
  - ex) 실제 날씨가 맑은 날 중에서 모델이 맑다고 예측한 비율을 나타낸 지표

- Precision(=정밀도)
  - ‘모델이 P라고 분류한 데이터 중에서’ 실제로 P인 데이터의 비율
  - `TP/(TP+FP)`
  - ex) 날씨 에측 모델이 '맑음'이라고 예측했는데, 실제 날씨가 '맑음'인지 확인할때 보는 지표

- F1 Score
  - Precision과 Recall의 조화 평균
  - `2 * (Precision * Recall)/(Precision + Recall)`

<br>

### ROC(Receiver Operating Characteristic)

![image](https://user-images.githubusercontent.com/76996686/139584185-a0c87ca8-bba1-48b5-943b-5af74ca85313.png)


- 여러 임계값들을 기준으로 Recall-Fallout의 변화를 시각화한 것
  - Fallout(=FPR) = FP/(TN+FP), 모델이 실제 false data인데 True라고 잘못 예측한 비율
- x축이 FPR, y축이 TPR 이므로 curve가 y축에 가까울 수록 성능이 좋다고 평가

- AUC(Area Under Curve)
  - ROC 그래프를 수치화 한 것으로 곡선 아래 영역
  - 1에 가까울 수록 좋은 모델